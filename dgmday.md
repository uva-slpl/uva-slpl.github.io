---
layout: main
title: DGM day
menu: no
---

![logo](/img/events/dgmday/logo.png){: style="float: right; margin: 0px 20px; height: 180px" }
# Deep generative models in NLP


On March 22 2018 we are gathering to present and discuss work on deep generative models (DGMs) and their applications to natural language processing. 
Our schedule will include a tutorial on variational inference and DMGs, research talks, and posters.

**Registration**  we invite colleagues from UvA to [register](https://docs.google.com/forms/d/e/1FAIpQLSfSzXOnb_5_4CLPBBzTtthm-k2U_qkYe2h52IMwgDoQdY7M-w/viewform?usp=sf_link) for free (we have 30 seats)

**Location** Room REC E0.15 at Roetersstraat 11, 1018 WB Amsterdam

**Schedule** see below


<iframe src="https://calendar.google.com/calendar/embed?showTitle=0&amp;mode=AGENDA&amp;height=300&amp;wkst=1&amp;bgcolor=%23FFFFFF&amp;src=aci7h1ua23taamdbat5hu73h14%40group.calendar.google.com&amp;color=%23AB8B00&amp;ctz=Europe%2FAmsterdam" style="border-width:0" width="600" height="300" frameborder="0" scrolling="no"></iframe>


# Programme


**Tutorial**

[Variational Inference and Deep Generative Models](https://github.com/philschulz/VITutorial): We start with a tutorial on VI and DGMs which is also a dry-run for a tutorial to be presented at ACL2018. This is a 3h-long tutorial with a 30 minutes coffee break.

**Lunch and posters**

**Research talks I**

<details>
    <summary>
        Bryan Eikema: <i>A joint model for neural machine translation.</i>
    </summary>
    <font color="darkgray">
        TBA
    </font>
</details>
<details>
    <summary>
        Philip Schulz: <i>Latent variables for neural machine translation.</i>
    </summary>
    <font color="darkgray">
        TBA
    </font>
</details>
<details>
    <summary>
        Ke Tran: <i>Inducing Grammars with and for Neural Machine Translation.</i>
</summary>
    <font color="darkgray">
    Machine translation systems require semantic knowledge and grammatical understanding. Neural machine translation (NMT) systems often assume this information is captured by an attention mechanism and a decoder that ensures fluency.  Recent work has shown that incorporating explicit syntax alleviates the burden of modeling both types of knowledge. However, requiring parses is expensive and does not explore the question of what syntax a model needs during translation. To address both of these issues we introduce a model that simultaneously translates while inducing dependency trees.  In this way, we leverage the benefits of structure while investigating what syntax NMT must induce to maximize performance. We show that our dependency trees are 1. language pair dependent and 2. improve translation quality.
    </font>
</details>

<br>

**Coffee break**

**Research talks II**

<details>
    <summary>
        Miguel Rios: <i>Learning word representations by marginalisation of latent alignments.</i>
    </summary>
    <font color="darkgray">
    TBA
    </font>
</details>
<details>
    <summary>
        Wilker Aziz: <i>Implicit models.</i>
    </summary>
    <font color="darkgray">
        <strong>TBA</strong><i></i>
    </font>
</details>
<details>
    <summary>
        <i>TBA</i>
    </summary>
    <font color="darkgray">
        TBA
    </font>
</details>

<br>

**Poster sessions with snacks and drinks**

* Implicit language models
* Learning from monolingual data with REINFORCE
* Latent graphs and permutations for neural machine translation 
* Natural language inference for multiple domains
* TBA
* TBA
* TBA
* TBA

# Contact

The [SLPL lab](https://staff.fnwi.uva.nl/k.simaan/research_all.html) is hosting this event.

Organisers:

* Wilker Aziz
* Miguel Rios
* Philip Schulz
* Khalil Sima'an

Contact persons: [Wilker](mailto:w.aziz@uva.nl) and [Miguel](mailto:m.riosgaona@uva.nl)


This event is supported by the Dutch Organization for Scientific Research (NWO) VICI Grant nr. 277-89-002.

